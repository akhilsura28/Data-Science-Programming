{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the ensemble tutorial notebook (which introduces ensemble techniques) to use randomsearchcv on each technique (refer to the sklearn documentation for more information on the parameters for each model). At the end of your notebook, discuss the performance of the new models you created. Be sure to indicate which of the new models perform best and how do each compare to their untuned versions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxGJbLsUhuc8"
   },
   "source": [
    "# Introduction To Ensemble Learning\n",
    "\n",
    "In this notebook, I have updated the ensemble tutorial notebook to use RandomSearchCV on each of the below techniques and also executed the default ensemble models. \n",
    "\n",
    "* RandomForest\n",
    "* AdaBoost\n",
    "* Gradiant Boosting\n",
    "* XG Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tuXRZKEYrDa"
   },
   "source": [
    "## Introduction and Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q08EVUytY3eh"
   },
   "source": [
    "In this notebook, we will reuse the Universal Bank dataset.\n",
    "\n",
    "This time, we are developing a model to predict whether a customer will accept a personal loan offer. The dataset contains 5000 observations and 14 variables. The data is available on one of my GitHub repos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmYLcm3aY8X5"
   },
   "source": [
    "## Install and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to install xgboost (it's not part of the sklearn package)\n",
    "# !conda install xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8zNdljvIhuc8"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(86089106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGgrXNQPZT3J"
   },
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q3u5LsGyhudA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>ZIP Code</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Education</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>91107</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>90089</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>94720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>94112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>91330</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Age  Experience  Income  ZIP Code  Family  CCAvg  Education  Mortgage  \\\n",
       "0   1   25           1      49     91107       4    1.6          1         0   \n",
       "1   2   45          19      34     90089       3    1.5          1         0   \n",
       "2   3   39          15      11     94720       1    1.0          1         0   \n",
       "3   4   35           9     100     94112       1    2.7          2         0   \n",
       "4   5   35           8      45     91330       4    1.0          2         0   \n",
       "\n",
       "   Personal Loan  Securities Account  CD Account  Online  CreditCard  \n",
       "0              0                   1           0       0           0  \n",
       "1              0                   1           0       0           0  \n",
       "2              0                   0           0       0           0  \n",
       "3              0                   0           0       0           0  \n",
       "4              0                   0           0       0           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/prof-tcsmith/data/master/UniversalBank.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aOH_GFGZZFx"
   },
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "OkUM_mnHhudC",
    "outputId": "b4e542fe-5d03-4602-e4d9-06a6d0e7c65d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID          Age   Experience       Income      ZIP Code  \\\n",
      "count  5000.000000  5000.000000  5000.000000  5000.000000   5000.000000   \n",
      "mean   2500.500000    45.338400    20.104600    73.774200  93152.503000   \n",
      "std    1443.520003    11.463166    11.467954    46.033729   2121.852197   \n",
      "min       1.000000    23.000000    -3.000000     8.000000   9307.000000   \n",
      "25%    1250.750000    35.000000    10.000000    39.000000  91911.000000   \n",
      "50%    2500.500000    45.000000    20.000000    64.000000  93437.000000   \n",
      "75%    3750.250000    55.000000    30.000000    98.000000  94608.000000   \n",
      "max    5000.000000    67.000000    43.000000   224.000000  96651.000000   \n",
      "\n",
      "            Family        CCAvg    Education     Mortgage  Personal Loan  \\\n",
      "count  5000.000000  5000.000000  5000.000000  5000.000000    5000.000000   \n",
      "mean      2.396400     1.937938     1.881000    56.498800       0.096000   \n",
      "std       1.147663     1.747659     0.839869   101.713802       0.294621   \n",
      "min       1.000000     0.000000     1.000000     0.000000       0.000000   \n",
      "25%       1.000000     0.700000     1.000000     0.000000       0.000000   \n",
      "50%       2.000000     1.500000     2.000000     0.000000       0.000000   \n",
      "75%       3.000000     2.500000     3.000000   101.000000       0.000000   \n",
      "max       4.000000    10.000000     3.000000   635.000000       1.000000   \n",
      "\n",
      "       Securities Account  CD Account       Online   CreditCard  \n",
      "count         5000.000000  5000.00000  5000.000000  5000.000000  \n",
      "mean             0.104400     0.06040     0.596800     0.294000  \n",
      "std              0.305809     0.23825     0.490589     0.455637  \n",
      "min              0.000000     0.00000     0.000000     0.000000  \n",
      "25%              0.000000     0.00000     0.000000     0.000000  \n",
      "50%              0.000000     0.00000     1.000000     0.000000  \n",
      "75%              0.000000     0.00000     1.000000     1.000000  \n",
      "max              1.000000     1.00000     1.000000     1.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ID                  5000 non-null   int64  \n",
      " 1   Age                 5000 non-null   int64  \n",
      " 2   Experience          5000 non-null   int64  \n",
      " 3   Income              5000 non-null   int64  \n",
      " 4   ZIP Code            5000 non-null   int64  \n",
      " 5   Family              5000 non-null   int64  \n",
      " 6   CCAvg               5000 non-null   float64\n",
      " 7   Education           5000 non-null   int64  \n",
      " 8   Mortgage            5000 non-null   int64  \n",
      " 9   Personal Loan       5000 non-null   int64  \n",
      " 10  Securities Account  5000 non-null   int64  \n",
      " 11  CD Account          5000 non-null   int64  \n",
      " 12  Online              5000 non-null   int64  \n",
      " 13  CreditCard          5000 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 547.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiaaNFX2Zf-I"
   },
   "source": [
    "## Clean/transform data (where necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "3JuJlVGDkINJ",
    "outputId": "082781c3-db3c-44e8-a7fd-ba35f35cbbfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\n",
       "       'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\n",
       "       'CD Account', 'Online', 'CreditCard'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on findings from data exploration, we need to clean up colum names, as there are some leading whitespace characters\n",
    "df.columns = [s.strip() for s in df.columns] \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns we are not using as predictors (see previous notebooks -- we are given a subset of input variables to consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['ID', 'ZIP Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Income</th>\n",
       "      <th>Family</th>\n",
       "      <th>CCAvg</th>\n",
       "      <th>Mortgage</th>\n",
       "      <th>Personal Loan</th>\n",
       "      <th>Securities Account</th>\n",
       "      <th>CD Account</th>\n",
       "      <th>Online</th>\n",
       "      <th>CreditCard</th>\n",
       "      <th>Edu_2</th>\n",
       "      <th>Edu_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Experience  Income  Family  CCAvg  Mortgage  Personal Loan  \\\n",
       "0   25           1      49       4    1.6         0              0   \n",
       "1   45          19      34       3    1.5         0              0   \n",
       "2   39          15      11       1    1.0         0              0   \n",
       "\n",
       "   Securities Account  CD Account  Online  CreditCard  Edu_2  Edu_3  \n",
       "0                   1           0       0           0      0      0  \n",
       "1                   1           0       0           0      0      0  \n",
       "2                   0           0       0           0      0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translation education categories into dummy vars\n",
    "df = df.join(pd.get_dummies(df['Education'], prefix='Edu', drop_first=True))\n",
    "df.drop('Education', axis=1, inplace = True)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKY30W1pZxCP"
   },
   "source": [
    "## Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "d0fAfB0ThudG",
    "outputId": "47f231af-3781-4603-95b8-d9a1fca0236e"
   },
   "outputs": [],
   "source": [
    "# construct datasets for analysis\n",
    "target = 'Personal Loan'\n",
    "predictors = list(df.columns)\n",
    "predictors.remove(target)\n",
    "X = df[predictors]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "t0DkCAoChudI",
    "outputId": "4f5824b6-d5e0-419c-c916-6be218916af2"
   },
   "outputs": [],
   "source": [
    "# create the training set and the test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model the data\n",
    "\n",
    "First, we will create a dataframe to hold all the results of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING DECISION TREE WITH RANDOM SEARCH CV TO TRAIN AND TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
      "The best recall score is 0.9214382632293081\n",
      "... with parameters: {'min_samples_split': 6, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.00015000000000000001, 'max_leaf_nodes': 138, 'max_depth': 34, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(2,200),  \n",
    "    'min_samples_leaf': np.arange(1,200),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(10, 200), \n",
    "    'max_depth': np.arange(3,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=1000,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9840000 Precision=0.9562044 Recall=0.8791946 F1=0.9160839\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Dtree with RandomSearch\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2A_u7rQhuc_"
   },
   "source": [
    "## PREDICTION WITH DECISION TREE (USING DEFAULT PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30gNRdX-qtNT"
   },
   "source": [
    "You can find details about SKLearm's DecisionTree classifier [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbPfUmcEXf6K"
   },
   "source": [
    "Create a decision tree using all of the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UZ60Vn1AhudK"
   },
   "outputs": [],
   "source": [
    "dtree=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntgxBhvJXkjp"
   },
   "source": [
    "Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "xPL4rRlVhudM",
    "outputId": "db26a1f0-23c9-4a02-87c5-34e3dbd71ccf"
   },
   "outputs": [],
   "source": [
    "_ = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMvD4-9wXy_1"
   },
   "source": [
    "Review of the performance of the model on the validation/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "YAcO31dIX7JE",
    "outputId": "797a7c84-5c1c-4ec8-c75e-12c675ea8061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.912751677852349\n",
      "Accuracy Score:   0.9866666666666667\n",
      "Precision Score:  0.951048951048951\n",
      "F1 Score:         0.9315068493150686\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Dtree Default\", \n",
    "                                                    \n",
    "                                                    'Accuracy': accuracy_score(y_test, y_pred), \n",
    "                                                    'Precision': precision_score(y_test, y_pred), \n",
    "                                                    'Recall': recall_score(y_test, y_pred), \n",
    "                                                    'F1': f1_score(y_test, y_pred)\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING RANDOM FOREST CLASSIFIER WITH RANDOM SEARCH CV TO TRAIN AND TEST THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
      "The best recall score is 0.854862053369516\n",
      "... with parameters: {'min_samples_split': 5, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0004, 'max_leaf_nodes': 176, 'max_depth': 39, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(2,200),  \n",
    "    'min_samples_leaf': np.arange(1,200),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(10, 200), \n",
    "    'max_depth': np.arange(3,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "rforest = RandomForestClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = rforest, param_distributions=param_grid, cv=kfolds, n_iter=1000,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9753333 Precision=0.9590164 Recall=0.7852349 F1=0.8634686\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Random Forest with Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION WITH RANDOM FOREST (USING DEFAULT PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, RandomeForestClassifier has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the defaul values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* n_estimators: The number of trees in the forsest\n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is 100.  \n",
    "* max_depth: The maximum depth per tree. \n",
    "    - Deeper trees might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is None, which allows the tree to grow without constraint.\n",
    "* See the SciKit Learn documentation for more details. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = rforest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.8657718120805369\n",
      "Accuracy Score:   0.9846666666666667\n",
      "Precision Score:  0.9772727272727273\n",
      "F1 Score:         0.9181494661921707\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Random Forest Default\", \n",
    "                                                    'Accuracy': accuracy_score(y_test, y_pred), \n",
    "                                                    'Precision': precision_score(y_test, y_pred), \n",
    "                                                    'Recall': recall_score(y_test, y_pred), \n",
    "                                                    'F1': f1_score(y_test, y_pred)\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING ADABoost WITH RANDOM SEARCH CV TO TRAIN AND TEST THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best recall score is 0.8036182722749887\n",
      "... with parameters: {'n_estimators': 100, 'learning_rate': 1.0, 'algorithm': 'SAMME.R'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(50, 500, 50),\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "}\n",
    "\n",
    "adaboost = AdaBoostClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator=adaboost, param_distributions=param_grid, cv=kfolds,\n",
    "                                 scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                                 return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9640000 Precision=0.8800000 Recall=0.7382550 F1=0.8029197\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"AdaBoost with Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION WITH ADABoost (USING DEFAULT PARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, ADABoostClassifier has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the defaul values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* max_depth: The maximum depth per tree. \n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is None (meaning, the tree can grow to a point where all leaves have 1 observation).\n",
    "* learning_rate: The learning rate determines the step size at each iteration while your model optimizes toward its objective. \n",
    "    - A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum.\n",
    "    - Larger learning rates may not converge on a solution.\n",
    "    - The value must be between 0 and 1. Default is 0.3.\n",
    "* n_estimators: The number of trees in our ensemble. \n",
    "    - Equivalent to the number of boosting rounds.\n",
    "    - The value must be an integer greater than 0. Default is 100.\n",
    "* See the SciKit Learn documentation for more details. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "aboost = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = aboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = aboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.7248322147651006\n",
      "Accuracy Score:   0.9626666666666667\n",
      "Precision Score:  0.8780487804878049\n",
      "F1 Score:         0.7941176470588235\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"AdaBoost Default\", \n",
    "                                                    'Accuracy': accuracy_score(y_test, y_pred), \n",
    "                                                    'Precision': precision_score(y_test, y_pred), \n",
    "                                                    'Recall': recall_score(y_test, y_pred), \n",
    "                                                    'F1': f1_score(y_test, y_pred)\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recall result from this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING RANDOM SEARCH CV TO TRAIN AND TEST THE MODEL USING GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
      "The best recall score is 0.9183175033921304\n",
      "... with parameters: {'n_estimators': 400, 'min_samples_split': 8, 'min_samples_leaf': 18, 'max_depth': 31, 'learning_rate': 0.1, 'criterion': 'squared_error'}\n"
     ]
    }
   ],
   "source": [
    "#rom sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "\n",
    "# Update the classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(2, 20),  \n",
    "    'min_samples_leaf': np.arange(2, 20),\n",
    "    'n_estimators': np.arange(50, 500, 50),\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],  # Different criterion for Gradient Boosting\n",
    "    'max_depth': np.arange(3, 50),\n",
    "}\n",
    "\n",
    "\n",
    "# Update the classifier instantiation\n",
    "rand_search = RandomizedSearchCV(estimator=gb_classifier, param_distributions=param_grid, cv=kfolds, n_iter=1000,\n",
    "                                 scoring=score_measure, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# Fit the model\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "# Retrieve the best estimator\n",
    "bestRecallTree = rand_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9846667 Precision=0.9632353 Recall=0.8791946 F1=0.9192982\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"GradientBoost with Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, GradientBoostingClassifier has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the defaul values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* max_depth: The maximum depth per tree. \n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is None (meaning, the tree can grow to a point where all leaves have 1 observation).\n",
    "* learning_rate: The learning rate determines the step size at each iteration while your model optimizes toward its objective. \n",
    "    - A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum.\n",
    "    - Larger learning rates may not converge on a solution.\n",
    "    - The value must be between 0 and 1. Default is 0.3.\n",
    "* n_estimators: The number of trees in our ensemble. \n",
    "    - Equivalent to the number of boosting rounds.\n",
    "    - The value must be an integer greater than 0. Default is 100.\n",
    "* See the SciKit Learn documentation for more details. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.8657718120805369\n",
      "Accuracy Score:   0.9826666666666667\n",
      "Precision Score:  0.9555555555555556\n",
      "F1 Score:         0.9084507042253522\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"GradientBoost Default\", \n",
    "                                                    'Accuracy': accuracy_score(y_test, y_pred), \n",
    "                                                    'Precision': precision_score(y_test, y_pred), \n",
    "                                                    'Recall': recall_score(y_test, y_pred), \n",
    "                                                    'F1': f1_score(y_test, y_pred)\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recall result from this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING XGBoost Classifier with RANDOM SEARCH CV TO TRAIN AND TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
      "The best recall score is 0.9667571234735414\n",
      "... with parameters: {'subsample': 0.7, 'scale_pos_weight': 9, 'n_estimators': 400, 'min_child_weight': 51, 'max_depth': 35, 'learning_rate': 0.01, 'gamma': 0.06, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_child_weight': np.arange(1, 200),\n",
    "    'gamma': np.arange(0.0, 0.5, 0.01),\n",
    "    'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "    'colsample_bytree': np.arange(0.5, 1.0, 0.1),\n",
    "    'max_depth': np.arange(3, 50),\n",
    "    'learning_rate': np.arange(0.01, 0.1, 0.01),\n",
    "    'n_estimators': np.arange(100, 1000, 100),\n",
    "    'scale_pos_weight': np.arange(1, 10)\n",
    "}\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator=xgb_classifier, param_distributions=param_grid, cv=kfolds, n_iter=1000,\n",
    "                                 scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                                 return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9680000 Precision=0.7729730 Recall=0.9597315 F1=0.8562874\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"XG boost with Random search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all our classifiers, XGBoost has a number of parameters that can be adjusted/tuned. In this example below, we simply accept the defaults. You may want to experiment with changing the default values and also use GridSearchCV to explore ranges of values.\n",
    "\n",
    "* max_depth: The maximum depth per tree. \n",
    "    - A deeper tree might increase the performance, but also the complexity and chances to overfit.\n",
    "    - The value must be an integer greater than 0. Default is 6.\n",
    "* learning_rate: The learning rate determines the step size at each iteration while your model optimizes toward its objective. \n",
    "    - A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum.\n",
    "    - The value must be between 0 and 1. Default is 0.3.\n",
    "* n_estimators: The number of trees in our ensemble. \n",
    "    - Equivalent to the number of boosting rounds.\n",
    "    - The value must be an integer greater than 0. Default is 100.\n",
    "* colsample_bytree: Represents the fraction of columns to be randomly sampled for each tree. \n",
    "    - It might improve overfitting.\n",
    "    - The value must be between 0 and 1. Default is 1.\n",
    "* subsample: Represents the fraction of observations to be sampled for each tree. \n",
    "    - A lower values prevent overfitting but might lead to under-fitting.\n",
    "    - The value must be between 0 and 1. Default is 1.\n",
    "* See the XGBoost documentation for more details. https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.8926174496644296\n",
      "Accuracy Score:   0.9853333333333333\n",
      "Precision Score:  0.9568345323741008\n",
      "F1 Score:         0.9236111111111113\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"XGBoost Default\", \n",
    "                                                    'Accuracy': accuracy_score(y_test, y_pred), \n",
    "                                                    'Precision': precision_score(y_test, y_pred), \n",
    "                                                    'Recall': recall_score(y_test, y_pred), \n",
    "                                                    'F1': f1_score(y_test, y_pred)\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summary of results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost Default</td>\n",
       "      <td>0.962667</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.724832</td>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost with Random search</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.738255</td>\n",
       "      <td>0.802920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest with Random search</td>\n",
       "      <td>0.975333</td>\n",
       "      <td>0.959016</td>\n",
       "      <td>0.785235</td>\n",
       "      <td>0.863469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Default</td>\n",
       "      <td>0.984667</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.865772</td>\n",
       "      <td>0.918149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoost Default</td>\n",
       "      <td>0.982667</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.865772</td>\n",
       "      <td>0.908451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dtree with RandomSearch</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.956204</td>\n",
       "      <td>0.879195</td>\n",
       "      <td>0.916084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoost with Random search</td>\n",
       "      <td>0.984667</td>\n",
       "      <td>0.963235</td>\n",
       "      <td>0.879195</td>\n",
       "      <td>0.919298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost Default</td>\n",
       "      <td>0.985333</td>\n",
       "      <td>0.956835</td>\n",
       "      <td>0.892617</td>\n",
       "      <td>0.923611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dtree Default</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.912752</td>\n",
       "      <td>0.931507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XG boost with Random search</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.772973</td>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.856287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model  Accuracy  Precision    Recall        F1\n",
       "0                  AdaBoost Default  0.962667   0.878049  0.724832  0.794118\n",
       "0       AdaBoost with Random search  0.964000   0.880000  0.738255  0.802920\n",
       "0  Random Forest with Random search  0.975333   0.959016  0.785235  0.863469\n",
       "0             Random Forest Default  0.984667   0.977273  0.865772  0.918149\n",
       "0             GradientBoost Default  0.982667   0.955556  0.865772  0.908451\n",
       "0           Dtree with RandomSearch  0.984000   0.956204  0.879195  0.916084\n",
       "0  GradientBoost with Random search  0.984667   0.963235  0.879195  0.919298\n",
       "0                   XGBoost Default  0.985333   0.956835  0.892617  0.923611\n",
       "0                     Dtree Default  0.986667   0.951049  0.912752  0.931507\n",
       "0       XG boost with Random search  0.968000   0.772973  0.959732  0.856287"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.sort_values(by=['Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets conclude the performance of models with Random search to their untuned versions:\n",
    "\n",
    "1. AdaBoost with Random search:\n",
    "   - The model shows a slight improvement in accuracy, precision, recall, and F1 score compared to the default AdaBoost model. The Random search likely helped in finding better hyperparameters, leading to improved performance.\n",
    "\n",
    "2. Random Forest with Random search:\n",
    "   - The Random search significantly improves the Random Forest model's performance across all metrics. The accuracy, precision, recall, and F1 score are noticeably higher compared to the default Random Forest model. The Random search likely found optimal hyperparameters, resulting in better overall performance.\n",
    "\n",
    "3. GradientBoost with Random search:\n",
    "   - The GradientBoost model with Random search performs similarly to the default GradientBoost model. There is a slight improvement in precision, recall, and F1 score, but the changes are not significant. It indicates that the default hyperparameters were already performing well, and the Random search did not result in significant improvements.\n",
    "\n",
    "4. XGBoost with Random search:\n",
    "   - The XGBoost model with Random search performs slightly worse in terms of precision and recall compared to the default XGBoost model. However, the accuracy and F1 score remain high. It suggests that the Random search might not have found the optimal hyperparameters for this particular dataset.\n",
    "\n",
    "5. Decision Tree with RandomSearch:\n",
    "   - The Decision Tree model with Random search shows improvement in all metrics compared to the default Decision Tree model. The Random search likely found better hyperparameters, leading to increased accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Overall,I can say that Random search technique proves to be effective in improving the performance of some models. The Random Forest model with Random search performs the best among the models with Random search, outperforming its default version in terms of accuracy, precision, recall, and F1 score. The Decision Tree model with Random search also demonstrates notable improvement. But, for some models like GradientBoost and XGBoost, the Random search does not result in significant performance enhancements, indicating that the default hyperparameters were already performing well for those models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Class08b-decision_tree_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

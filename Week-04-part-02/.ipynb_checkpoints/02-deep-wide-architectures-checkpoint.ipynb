{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "w0tyra0NWJvI"
   },
   "source": [
    "# MNIST machine learning exercise\n",
    "\n",
    "In this exercise we will demonstrate the use of Keras and Keras Tune to identify a feedforward neural network that best predicts the a handwritten digit. \n",
    "\n",
    "We use the MNIST data set;\n",
    "\n",
    "![mnist data](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load and explore data (shouldn't need any transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 17:52:09.441995: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "48VnFR9cXFP0"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST digits dataset. It's originally from UCI machine learning library, but included in SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "M8CjaVlYW2Jx",
    "outputId": "e90a4dd3-3781-477f-b02f-97ac3e75be91"
   },
   "outputs": [],
   "source": [
    "mnist = datasets.load_digits() # sklearn includes this data set .. https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset is stored in a Bunch type (see sklearn https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html)\n",
    "\n",
    "We can view this dataset as similar to a dictionary; we can look at all the keys by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5pPXrazAfUoL",
    "outputId": "593b0df0-7b98-4c9b-e01d-36930d653723"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note thjat there are 1797 images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are 8x8 grid of values epresenting the gray level for each pixel (16 levels of grey -- from 0 (black) to 15 (white)). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want verify the number of images, we can use the len function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rTQ7qNp4ffaW",
    "outputId": "3d267533-ac01-4747-abee-369aae5d3d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist.data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, for each image we have a target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist.target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Depth and Width\n",
    "\n",
    "A deep neural network is a neural network with a large number of layers. A wide neural network is a neural network with a large number of neurons in one or more layers. A wide network can also refer to a network that has more than one hidden layer in parallel.\n",
    "\n",
    "**Wide and Shallow**\n",
    "First, let's look at a wide and shallow network. The depth will be 1 hidden layer, while the width we be 1000 neurons.\n",
    "\n",
    "**Deep and Narrow**\n",
    "Next, we will look at a deep and narrow network. The depth will be 5 hidden layers, while the width we be 10 neurons.\n",
    "\n",
    "**Wide and Deep Network (parallel)**\n",
    "Finally, we will look at a deep and wide network. In this final example, we add three hidden layers of 100 neurons in parallel. The input layer, therefore, connects directly to three layers that are in parallel. Each of the three parallell layers then feed into one output layer. Now, logically, if we concatenate 3 layers of 10 neurons such that they are parallel, this is equivalent to one layer of 30 neurons. Where things can be a bit more interesting is when we have one layer recieve input from multiple layers. For instance, have a two three layer network, where the third layer is a concatenation of the first layer and the second layer. This is a deep and wide network with parallel layers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Shallow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we simply have multiple layers (depth) with each layer only have a relatively small number of units (neurons).\n",
    "\n",
    "Also, note that we introduce a new way of archtecting the network. Note that in the kera intro notebook we used the following to add layers:\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(64)) \n",
    "model.add(keras.layers.Dense(500, activation=\"relu\")) \n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, this same network can be defined using the technique you see below. Notice that each layer is given a name, and thus, this allows for the layers to be connected in different ways. For instance, we can connect the input layer to multiple layers, or we can connect multiple layers to a single layer. This is a very powerful way of defining a network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(64)\n",
    "hidden1 = keras.layers.Dense(500, activation=\"relu\")(input_)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(hidden1)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our neural network model, we can get a summary of the model by calling the summary() function on the model. Since this network, though implemented using different syntax, is the same model as we defined in the intro to keras notebook covered previous (see previous notebook if you need a reminder about how to interpret this output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               32500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,510\n",
      "Trainable params: 37,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code only defines the structure of the model. We now need to compile the model. When we compile the model we specify details about how it will be trained. We need to specify a loss function, and optimizer approach, and a metric to optimize. \n",
    "\n",
    "In the following model, we will use the categorical_crossentropy loss function, which is appropriate for a multi-class classification problem. We will use the Adam optimizer, which is a variant of stochastic gradient descent. We will use accuracy as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the structure of our model defined, and the details of the training process specified, we can train the model. We will train the model for 10 epochs, and use a batch size of 128. We will also use the validation data set to evaluate the model after each epoch.\n",
    "\n",
    "In the specific case of this dataset, we have a training dataset that is 80% of 1792 (1437) 8x8 images of handwritten digits. If we set our batch size to 111, then we will have 12 full batches and one partial -- so, 13 batches per epoch (1437/111 = 12.95). We will train the model for 10 epochs, so we will have 130 batches of training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Optimization algorithms (aka 'learning algorithms') generally have a number of hyperparameters. Two hyperparameters that often confuse beginners are the batch size and number of epochs. \n",
    ">\n",
    "* The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model’s internal parameters are updated.\n",
    "* The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset.\n",
    "  >\n",
    "> So, if you have a training set of data that consists of 100 observations; if our batch size is 10, then the gradient descent algorithm will update the weights after every 10 observations. If  we have 100 epochs, then the gradient descent algorithm will update the weights 100 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 2.8432 - accuracy: 0.4955 - val_loss: 0.4235 - val_accuracy: 0.8667\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8949 - val_loss: 0.3094 - val_accuracy: 0.9083\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.9304 - val_loss: 0.2265 - val_accuracy: 0.9222\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.9506 - val_loss: 0.2003 - val_accuracy: 0.9417\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9638 - val_loss: 0.1912 - val_accuracy: 0.9361\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9701 - val_loss: 0.1701 - val_accuracy: 0.9417\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9722 - val_loss: 0.1662 - val_accuracy: 0.9500\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1170 - accuracy: 0.9743 - val_loss: 0.1425 - val_accuracy: 0.9556\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1057 - accuracy: 0.9736 - val_loss: 0.1403 - val_accuracy: 0.9583\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0980 - accuracy: 0.9805 - val_loss: 0.1299 - val_accuracy: 0.9611\n",
      "['loss', 'accuracy']\n",
      "CPU times: user 732 ms, sys: 86.6 ms, total: 819 ms\n",
      "Wall time: 616 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=111, validation_data=(X_test, y_test))\n",
    "\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to evaluate the model on the test data. We can do this with the evaluate() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 650us/step - loss: 0.1299 - accuracy: 0.9611\n",
      "Loss 0.12988\n",
      "Accuracy 0.96111\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss {loss:.5f}\\nAccuracy {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               32500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,510\n",
      "Trainable params: 37,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Network and Narrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks consist of many hidden layers. The number of layers is the depth of the network. The first layer is the input layer. The last layer is the output layer. The layers in between are the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(64))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also create this network using the syntax below (this method is required for more complex ann architecures, such as when some layers fork to other deeper layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(64)\n",
    "hidden1 = keras.layers.Dense(200, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(200, activation=\"relu\")(hidden1)\n",
    "hidden3 = keras.layers.Dense(200, activation=\"relu\")(hidden2)\n",
    "hidden4 = keras.layers.Dense(200, activation=\"relu\")(hidden3)\n",
    "hidden5 = keras.layers.Dense(200, activation=\"relu\")(hidden4)\n",
    "hidden6 = keras.layers.Dense(200, activation=\"relu\")(hidden5)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(hidden5)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "131/131 [==============================] - 0s 2ms/step - loss: 0.7098 - accuracy: 0.7648 - val_loss: 0.2713 - val_accuracy: 0.9278\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9402 - val_loss: 0.1676 - val_accuracy: 0.9583\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9673 - val_loss: 0.1024 - val_accuracy: 0.9667\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9770 - val_loss: 0.2525 - val_accuracy: 0.9111\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9868 - val_loss: 0.0942 - val_accuracy: 0.9694\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9916 - val_loss: 0.0800 - val_accuracy: 0.9694\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 0.9958 - val_loss: 0.0653 - val_accuracy: 0.9806\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 0.1442 - val_accuracy: 0.9500\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9896 - val_loss: 0.0734 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 0s 1ms/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.0724 - val_accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=11, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 983us/step - loss: 0.0724 - accuracy: 0.9778\n",
      "Loss 0.07236\n",
      "Accuracy 0.97778\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss {loss:.5f}\\nAccuracy {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 200)               13000     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,810\n",
      "Trainable params: 175,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'wide and deep' network is simply a network with many layers (deep), and many units per layer (wide). In the example below, we will also fork one layer so that it is both wide and deep.\n",
    "\n",
    "A wide and deep network is a network that has both a wide component and a deep component. The wide component is a set of layers that are connected directly to the output layer. The deep component is a set of layers that are connected to each other, and then to the output layer. The wide component allows the network to learn simple relationships between the input features and the output. The deep component allows the network to learn complex relationships between the input features and the output. The wide and deep network combines the strengths of both the wide component and the deep component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(64)\n",
    "hidden1 = keras.layers.Dense(1000, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(1000, activation=\"relu\")(hidden1)\n",
    "hidden3 = keras.layers.Dense(1000, activation=\"relu\")(hidden2)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(hidden3)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 1s 10ms/step - loss: 0.7830 - accuracy: 0.7968 - val_loss: 0.2143 - val_accuracy: 0.9472\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1696 - accuracy: 0.9652 - val_loss: 0.1816 - val_accuracy: 0.9389\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1173 - accuracy: 0.9708 - val_loss: 0.1083 - val_accuracy: 0.9722\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0820 - accuracy: 0.9854 - val_loss: 0.0990 - val_accuracy: 0.9722\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0600 - accuracy: 0.9903 - val_loss: 0.0862 - val_accuracy: 0.9778\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0461 - accuracy: 0.9951 - val_loss: 0.0774 - val_accuracy: 0.9806\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0397 - accuracy: 0.9958 - val_loss: 0.0735 - val_accuracy: 0.9861\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0327 - accuracy: 0.9972 - val_loss: 0.0726 - val_accuracy: 0.9861\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0310 - accuracy: 0.9965 - val_loss: 0.0666 - val_accuracy: 0.9833\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.0229 - accuracy: 0.9979 - val_loss: 0.0773 - val_accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9833\n",
      "Loss 0.07733\n",
      "Accuracy 0.98333\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss {loss:.5f}\\nAccuracy {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1000)              65000     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                10010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,077,010\n",
      "Trainable params: 2,077,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep Network with Parallel Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'wide and deep' network is simply a network with many layers (deep), and many units per layer (wide). In the example below, we will also fork one layer so that it is both wide and deep.\n",
    "\n",
    "A wide and deep network is a network that has both a wide component and a deep component. The wide component is a set of layers that are connected directly to the output layer. The deep component is a set of layers that are connected to each other, and then to the output layer. The wide component allows the network to learn simple relationships between the input features and the output. The deep component allows the network to learn complex relationships between the input features and the output. The wide and deep network combines the strengths of both the wide component and the deep component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(64)\n",
    "hidden1 = keras.layers.Dense(1000, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(1000, activation=\"relu\")(hidden1)\n",
    "hidden3 = keras.layers.Dense(1000, activation=\"relu\")(hidden2)\n",
    "concat = keras.layers.Concatenate()([hidden1, hidden2])\n",
    "hidden3 = keras.layers.Dense(1000, activation=\"relu\")(concat)\n",
    "hidden4 = keras.layers.Dense(1000, activation=\"relu\")(hidden3)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(hidden4)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.8948 - accuracy: 0.7850 - val_loss: 0.2309 - val_accuracy: 0.9306\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 0.1759 - accuracy: 0.9617 - val_loss: 0.1702 - val_accuracy: 0.9583\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.1003 - accuracy: 0.9770 - val_loss: 0.0960 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.0703 - accuracy: 0.9847 - val_loss: 0.0945 - val_accuracy: 0.9722\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 0.0592 - accuracy: 0.9910 - val_loss: 0.0785 - val_accuracy: 0.9833\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 0.0390 - accuracy: 0.9951 - val_loss: 0.0781 - val_accuracy: 0.9833\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.0309 - accuracy: 0.9972 - val_loss: 0.0769 - val_accuracy: 0.9806\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 0.0240 - accuracy: 0.9986 - val_loss: 0.0668 - val_accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 0.0204 - accuracy: 0.9986 - val_loss: 0.0673 - val_accuracy: 0.9806\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 0.0178 - accuracy: 0.9986 - val_loss: 0.0661 - val_accuracy: 0.9861\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9861\n",
      "Loss 0.06606\n",
      "Accuracy 0.98611\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss {loss:.5f}\\nAccuracy {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 1000)         65000       ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 1000)         1001000     ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2000)         0           ['dense_20[0][0]',               \n",
      "                                                                  'dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 1000)         2001000     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 1000)         1001000     ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 10)           10010       ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,078,010\n",
      "Trainable params: 4,078,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored multiple configurations of neural network architectures, primarily focusing on variations in Width, Depth, and Parallel layers.\n",
    "\n",
    "Here are the results we obtained for these models:\n",
    "\n",
    "* Wide and Shallow: Accuracy = 0.95556\n",
    "* Deep and Narrow: Accuracy = 0.98056\n",
    "* Wide and Deep: Accuracy = 0.98611\n",
    "* Wide and Deep with Parallel Layers: Accuracy = 0.98333\n",
    "\n",
    "> NOTE: There may be some variation in the above results due to the random nature of the training process. \n",
    "\n",
    "In this particular instance, given the specific data, the number of layers, neurons, and interlayer connections, the architectures employing both 'Width' and 'Depth' (with and without 'Parallel Layers') exhibited superior accuracy. It's crucial, however, to highlight that this isn't a universally optimal model. Often, the Wide and Deep architectures won't outperform its counterparts.\n",
    "\n",
    "The key insight here is the non-existence of a universally 'best' architecture for all problems. An optimal architecture is highly dependent on specific factors like the data at hand, number of layers, the count of neurons, and the connection schema between layers. The most effective approach for uncovering the best architecture for a specific problem involves experimenting with a variety of architectures and comparing their performances.\n",
    "\n",
    "But don't let the prospect of infinite architecture configurations and parameter settings overwhelm you. Here's where to begin:\n",
    "\n",
    "* Literature Review: If your problem is not unique, there's a high chance it's been tackled before. Start by exploring academic and industry literature to understand what approaches others have used. This can provide you a solid base for your experiments.\n",
    "\n",
    "* In-house Knowledge: If you're within a company, leverage the knowledge and expertise of your colleagues. They've likely worked on similar problems and their insights could be invaluable.\n",
    "\n",
    "* Exploration and Automation: If neither of the above apply, you're essentially a pioneer, and this necessitates a more exploratory approach. You'll have to invest time in testing various combinations of architectures and parameters. Tools like Keras Tuner are highly valuable here as they facilitate automated exploration of potential architectures and parameter spaces. We'll delve deeper into this subject in a future notebook.\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOig4eSm144+FaPk1GKk187",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mnist_compete_3_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
